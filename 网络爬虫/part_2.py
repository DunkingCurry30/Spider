 ##################
 # 网络爬虫的尺寸 #
 ##################

#Requests库：小规模，数据量小，爬取速度不敏感，用于爬取网页，玩转网页

#Scrapy库：中规模，数据规模较大，爬取速度敏感，用于爬取网站（如携程），爬取系列网站

 ##################
 # 网络爬虫的限制 # 服务器网站对爬虫进行限制
 ##################

#来源审查：判断User-Agent进行限制：检查来访HTTP协议头的User-Agent域，只响应浏览器或友
########## 好爬虫的访问。

#Robots协议：告知爬虫可爬取的策略（哪些可以爬取，哪些不能爬取），需要爬虫遵守

 ##############
 # Robots协议 # 网站爬虫排除标准
 ##############

#基本语法：User-Agent:*  爬虫名称
#          Disallow: /   不允许爬取

#存放于网站的robots.txt文件中，例（http://www.jd.com/robots.txt)京东的robots协议

#使用：网络爬虫应该自动或人工识别robots.txt,在进行内容爬取

#约束性：建议但非约束性，可以不遵守但存在法律风险
